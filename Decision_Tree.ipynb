{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "cf891c1be4dc1ae06191273f05e0d68e",
          "grade": false,
          "grade_id": "cell-727395f9bc8b5c9d",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "vJSokESFe-ia"
      },
      "source": [
        "<h2>Implementing Regression Trees</h2>\n",
        "\n",
        "Import a few packages that you will need. In addition, you will load two binary classification dataset - the spiral dataset and the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a> dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "4ad00e0439033b41698216da17731123",
          "grade": false,
          "grade_id": "cell-5445d0afdbc4c054",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0XEi401e-ia",
        "outputId": "ba39fe97-8d9f-4f46-b592-f3a363d2c359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're running python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from scipy.stats import mode\n",
        "from google.colab import files\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pylab import *\n",
        "from numpy.matlib import repmat\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "import time\n",
        "import sys\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from helper import *\n",
        "\n",
        "print('You\\'re running python %s' % sys.version.split(' ')[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "7a36ccf5bfced96b28d8966f36446473",
          "grade": false,
          "grade_id": "cell-c219552fd154672d",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "tOjY1DO0e-ia"
      },
      "source": [
        "The code below generates spiral data using the trigonometric functions sine and cosine, then splits the data into train and test segments."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry9eZXuKgQbh",
        "outputId": "40ede640-7e86-4eb1-c401-a1737ea410b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Dataset.csv')\n",
        "df['diagnosis'] = df['diagnosis'].replace({'M': int('0')})\n",
        "df['diagnosis'] = df['diagnosis'].replace({'B': int('2')})\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XuExgtJgSij",
        "outputId": "a94d285d-e028-433e-bf9a-3561b1068954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xrawdata, Yrawdata = df.shape\n",
        "trpercent = 0.8"
      ],
      "metadata": {
        "id": "DP0UBsMDgU77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfxTr = df.iloc[:math.floor(Xrawdata*trpercent),2:32]\n",
        "dfyTr = df.iloc[:math.floor(Xrawdata*trpercent),1:2]\n",
        "dfxTe = df.iloc[math.floor(Xrawdata*trpercent):Xrawdata,2:32]\n",
        "dfyTe = df.iloc[math.floor(Xrawdata*trpercent):Xrawdata,1:2]"
      ],
      "metadata": {
        "id": "BPu9JLTYgVbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xTr = dfxTr.to_numpy()\n",
        "yTr = dfyTr.to_numpy()\n",
        "xTe = dfxTe.to_numpy()\n",
        "yTe = dfyTe.to_numpy()"
      ],
      "metadata": {
        "id": "zext1mg6gX2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1471471f16e7153e9f7916d8893fab91",
          "grade": false,
          "grade_id": "cell-c632b2561da0c25c",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "wf8mqja3e-ib"
      },
      "source": [
        "### Part One: Implement `sqimpurity` [Graded]\n",
        "\n",
        "First, implement the function **`sqimpurity`**, which takes as input a vector $y$ of $n$ labels and outputs the corresponding squared loss impurity:\n",
        "$$\n",
        "\\sum_{i = 1}^{n} \\left( y_i - \\overline{y} \\right)^2 \\textrm{, where } \\overline{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}.\n",
        "$$\n",
        "\n",
        "Again, the squared loss impurity works fine even though our final objective is classification. This is because the labels are binary and classification problems can be framed as regression problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "def430a6952c11c5e4cf8b3d9516fe81",
          "grade": false,
          "grade_id": "cell-ec2301f1325f79b5",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "WckDtiMYe-ib"
      },
      "outputs": [],
      "source": [
        "def sqimpurity(yTr):\n",
        "    \"\"\"\n",
        "    Computes the squared loss impurity (variance) of the labels.\n",
        "\n",
        "    Input:\n",
        "        yTr: n-dimensional vector of labels\n",
        "\n",
        "    Output:\n",
        "        squared loss impurity: weighted variance/squared loss impurity of the labels\n",
        "    \"\"\"\n",
        "\n",
        "    N = np.size(yTr)\n",
        "    assert N > 0 # must have at least one sample\n",
        "    impurity = 0\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "    impurity = np.sum((yTr - 1 / N * np.sum(yTr))**2)\n",
        "    return impurity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "110d11c6a9b86f38e65eb9d517778dac",
          "grade": false,
          "grade_id": "cell-c8238fba4c3de7ab",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "OO56Uyj2e-ic"
      },
      "source": [
        "Let's plot the shape of the impurity function. We vary the mixture of labels in a set of $n$ labels and calculate the impurity of the labels. When the labels are mostly the same, the impurity should be low. When the labels are evenly split between $+1$ and $-1$, the impurity should be the highest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "27c84f41b6f8c383bd75fccc138ad42c",
          "grade": false,
          "grade_id": "cell-4730d6e94ace8e06",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "17r3rJjge-ic"
      },
      "source": [
        "### Part Two: Implement `sqsplit` [Graded]\n",
        "\n",
        "Now implement **`sqsplit`**, which takes as input a data set of size $n \\times d$ with labels and computes the best feature and the threshold/cut of the optimal split based on the squared loss impurity. The function outputs a feature dimension `0 <= feature < d`, a cut threshold `cut`, and the impurity loss `bestloss` of this best split.\n",
        "\n",
        "Recall in the CART algorithm that, to find the split with the minimum impurity, you iterate over all features and cut values along each feature. We enforce that the cut value be the average of the two consecutive data points' feature values.\n",
        "\n",
        "You should calculate the impurity of a node of data $S$ with two branches $S_L$ and $S_R$ as:\n",
        "$$\n",
        "\\begin{align*}\n",
        "I(S) &= \\frac{\\left| S_L \\right|}{|S|} I \\left( S_L \\right) + \\frac{\\left| S_R \\right|}{|S|} I \\left( S_R \\right)\\\\\n",
        "&= \\frac{1}{|S|}\\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\frac{1}{|S|} \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\\\\\n",
        "&\\propto \\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "**Implementation Notes:**\n",
        "- For calculating the impurity of a node, you should just return the sum of left and right impurities instead of the average.\n",
        "- Returned `feature` must be 0-indexed as is consistent with programming in Python.\n",
        "- If along a feature $f$, two data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ have the same value, avoid splitting between them; move to the next pair of data points.\n",
        "\n",
        "For example, with the following `xTr` of size $4 \\times 3$ and `yTr` for 4 points:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 2\\\\\n",
        "2 & 0 & 1\\\\\n",
        "0 & 0 & 1\\\\\n",
        "2 & 1 & 2\n",
        "\\end{bmatrix},\n",
        "\\begin{bmatrix}\n",
        "1\\\\1\\\\1\\\\-1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "among possible features `[0, 1, 2]`, the best split would be at `feature = 1` and `cut = (0 + 1) / 2 = 0.5`.\n",
        "\n",
        "<hr>\n",
        "\n",
        "If you're stuck, we recommend that you start with the naïve algorithm for finding the best split, which involves a double loop over all features `0 <= f < d` and all cut values `xTr[0, f] < (xTr[i, f] + xTr[i+1, f]) / 2 < xTr[n-1, f]` (with `xTr` sorted along feature `f`). This algorithm thus calculates impurities for `d(n-1)` splits. Here's the pseudocode:\n",
        "\n",
        "<center><img src=\"cart-id3_best_split_pseudocode.png\" width=\"75%\" /></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "d9033ab9fb3dbbcb866dcc1bd45db8f4",
          "grade": false,
          "grade_id": "cell-sqsplit",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "1H24otITe-id"
      },
      "outputs": [],
      "source": [
        "def sqsplit(xTr, yTr):\n",
        "    \"\"\"\n",
        "    Finds the best feature, cut value, and impurity for a split of (xTr, yTr) based on squared loss impurity.\n",
        "\n",
        "    Input:\n",
        "        xTr: n x d matrix of data points\n",
        "        yTr: n-dimensional vector of labels\n",
        "\n",
        "    Output:\n",
        "        feature:  index of the best cut's feature (keep in mind this is 0-indexed)\n",
        "        cut:      cut-value of the best cut\n",
        "        bestloss: squared loss impurity of the best cut\n",
        "    \"\"\"\n",
        "    n, d = xTr.shape\n",
        "    assert d > 0 # must have at least one dimension\n",
        "    assert n > 1 # must have at least two samples\n",
        "\n",
        "    bestloss = np.inf\n",
        "    feature = np.inf\n",
        "    cut = np.inf\n",
        "\n",
        "    for col in range(d):\n",
        "        sort = xTr[:, col].argsort()\n",
        "        sortx = xTr[sort, col]\n",
        "        sorty = yTr[sort]\n",
        "\n",
        "        for row in range(n):\n",
        "            # if row + 1 exceeds range\n",
        "            if row + 1 == n:\n",
        "                continue\n",
        "            # check where values change\n",
        "            if sortx[row + 1] > sortx[row]:\n",
        "                Sl = sorty[:row+1]\n",
        "                Sr = sorty[row+1:]\n",
        "\n",
        "                loss = sqimpurity(Sl) + sqimpurity(Sr)\n",
        "\n",
        "                # if loss is smaller\n",
        "                if loss < bestloss:\n",
        "                    bestloss = loss\n",
        "                    feature = col\n",
        "                    cut = (sortx[row] + sortx[row + 1]) / 2\n",
        "\n",
        "    return feature, cut, bestloss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d358dfc0f25657b18024284428aec959",
          "grade": false,
          "grade_id": "cell-985b43cf6a0b1e16",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "tbviEHtbe-id"
      },
      "source": [
        "### Part Three: Implement `cart` [Graded]\n",
        "\n",
        "In this section, you will implement the function **`cart`**, which returns a regression tree based on the minimum squared loss splitting rule. You should use the function `sqsplit` to make your splits.\n",
        "\n",
        "**Implementation Notes:**\n",
        "We've provided a tree structure in the form of `TreeNode` for you that can be used for both leaves and nodes. To represent the leaves, you would set all fields except `prediction` to `None`.\n",
        "\n",
        "Non-leaf nodes will have non-`None` fields for all except `prediction`:\n",
        "1. `left`: node describing left subtree\n",
        "2. `right`: node describing right subtree\n",
        "3. `feature`: index of feature to cut (0-indexed as returned by `sqsplit`)\n",
        "4. `cut`: cutoff value $t$ ($\\leq t$: left and $> t$: right)\n",
        "5. `prediction`: `None`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "0903afbf0f71752d3ae3caa2a24e8360",
          "grade": false,
          "grade_id": "cell-919899ceb491184f",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "Uic5B4G_e-id"
      },
      "outputs": [],
      "source": [
        "class TreeNode(object):\n",
        "    \"\"\"\n",
        "    Tree class.\n",
        "\n",
        "    (You don't _need_ to add any methods or fields here but feel\n",
        "    free to if you like. The tests will only reference the fields\n",
        "    defined in the constructor below, so be sure to set these\n",
        "    correctly.)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, left, right, feature, cut, prediction):\n",
        "        # Check that all or no arguments are None\n",
        "        node_or_leaf_args = [left, right, feature, cut]\n",
        "        assert all([arg == None for arg in node_or_leaf_args]) or all([arg != None for arg in node_or_leaf_args])\n",
        "\n",
        "        # Check that all None <==> leaf <==> prediction not None\n",
        "        # Check that all non-None <==> non-leaf <==> prediction is None\n",
        "        if all([arg == None for arg in node_or_leaf_args]):\n",
        "            assert prediction is not None\n",
        "        if all([arg != None for arg in node_or_leaf_args]):\n",
        "            assert prediction is None\n",
        "\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.feature = feature\n",
        "        self.cut = cut\n",
        "        self.prediction = prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "9c2c5f1f300c14fa009977cd29781d7c",
          "grade": false,
          "grade_id": "cell-d6c63e37fd6c395f",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "5uqE0skwe-ie"
      },
      "source": [
        "Now implement the function `cart` using **recursion** (you call `cart` on the left and right subtrees inside the `cart` function). Recall the pseudocode for the CART algorithm.\n",
        "\n",
        "**NOTE:** In this implementation, you will be using **`np.mean`** for `prediction` argument. To check that floating point values in `xTr` are the same or not, you can use `np.isclose(xTr, xTr[0])`, which returns a list of `True` and `False` based on how different the rows of `xTr` are from the vector `xTr[0]`.\n",
        "\n",
        "<center><img src=\"cart-id3_pseudocode.png\" width=\"75%\" /></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "de3750e532687607733d83dcc9a0a2dc",
          "grade": false,
          "grade_id": "cell-cart",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "zDgykKcje-ie"
      },
      "outputs": [],
      "source": [
        "def cart(xTr, yTr):\n",
        "    \"\"\"\n",
        "    Builds a CART tree.\n",
        "\n",
        "    Input:\n",
        "        xTr:      n x d matrix of data\n",
        "        yTr:      n-dimensional vector\n",
        "\n",
        "    Output:\n",
        "        tree: root of decision tree\n",
        "    \"\"\"\n",
        "    n, d = xTr.shape\n",
        "    node = None\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "    prediction = np.mean(yTr)\n",
        "    index = np.arange(n)\n",
        "\n",
        "    # Base Case #1\n",
        "    if np.all(yTr == yTr[0]):\n",
        "        tree = TreeNode(None, None, None, None, yTr[0])\n",
        "    elif np.all(np.isclose(xTr, xTr[0])):\n",
        "        tree = TreeNode(None, None, None, None, prediction)\n",
        "    else:\n",
        "        feature, cut, loss = sqsplit(xTr, yTr)\n",
        "        left_index = index[xTr[:, feature] <= cut]\n",
        "        right_index = index[xTr[:, feature] > cut]\n",
        "        left_xTr = xTr[left_index, :]\n",
        "        right_xTr = xTr[right_index, :]\n",
        "        left_yTr = yTr[left_index]\n",
        "        right_yTr = yTr[right_index]\n",
        "\n",
        "        left_node = cart(left_xTr, left_yTr)\n",
        "        right_node = cart(right_xTr, right_yTr)\n",
        "        tree = TreeNode(left_node, right_node, feature, cut, None)\n",
        "    return tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "faf306bddae40ade7985dcf6439b861c",
          "grade": false,
          "grade_id": "cell-91bac7e5c5968bbf",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "ZEiPQy_1e-if"
      },
      "source": [
        "### Part Four: Implement `evaltree` [Graded]\n",
        "\n",
        "Implement the function **`evaltree`**, which evaluates a decision tree on a given test data set. You essentially need to traverse the tree until you end up in a leaf, where you return the `prediction` value of the leaf. Like the `cart` function, you can call `evaltree` on the left subtree and right subtree on testing points that fall in the corresponding subtrees.\n",
        "\n",
        "Here's some inspiration:\n",
        "1. If the `tree` is a leaf, i.e. the left and right subtrees are `None`, return `tree.prediction` for all `m` testing points.\n",
        "2. If the `tree` is non-leaf, using `tree.feature` and `tree.cut` find testing points with the feature value less than/equal to the threshold and greater than. Now, you can call `evaltree` on `tree.left` and the left set of testing points to obtain the left set's predictions. Then obtain the predictions for the right set, and return all predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "54f3f523c3529265fb0ce24f6fea9361",
          "grade": false,
          "grade_id": "cell-evaltree",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "NovuA2HEe-if"
      },
      "outputs": [],
      "source": [
        "def evaltree(tree, xTe):\n",
        "    \"\"\"\n",
        "    Evaluates testing points in xTe using decision tree root.\n",
        "\n",
        "    Input:\n",
        "        tree: TreeNode decision tree\n",
        "        xTe:  m x d matrix of data points\n",
        "\n",
        "    Output:\n",
        "        pred: m-dimensional vector of predictions\n",
        "    \"\"\"\n",
        "    m, d = xTe.shape\n",
        "    preds = np.zeros(m)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "\n",
        "    for x in range(m):\n",
        "        rt = tree\n",
        "        while(True):\n",
        "            if rt.left is None or rt.right is None:\n",
        "                preds[x] = rt.prediction\n",
        "                break\n",
        "            elif xTe[x,rt.feature] <= rt.cut:\n",
        "                rt = rt.left\n",
        "            else:\n",
        "                rt = rt.right\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Prediction = evaltree(cart(xTr, yTr),xTe)\n",
        "print(Prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djT8M3kyly8m",
        "outputId": "24cd9ef2-7557-4191-dca2-11a12464364c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 0. 0. 2. 0. 0. 0. 2. 2. 0. 2. 2. 0. 0. 2. 2. 2. 2. 2. 2. 0. 2. 2.\n",
            " 0. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 0. 2. 2. 2. 2. 2. 0. 0. 2. 0. 2.\n",
            " 0. 0. 0. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 0. 2. 2. 0. 0. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 0. 0. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(truth,preds):\n",
        "\n",
        "    truth = truth.flatten()\n",
        "    preds = preds.flatten()\n",
        "    diff = np.abs(truth - preds)\n",
        "    false = np.count_nonzero(diff)\n",
        "    corr = diff.shape[0] - false\n",
        "    accuracy = np.float64(corr / diff.shape[0])\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "Ey_LfCcFpGjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=accuracy(yTe,Prediction)\n",
        "t1 = time.time()\n",
        "print(\"You obtained %.2f%% classification acccuracy\" % (result*100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icntHGpBpSZr",
        "outputId": "6adf240f-d666-4721-a33b-7150c127d314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You obtained 83.33% classification acccuracy\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}